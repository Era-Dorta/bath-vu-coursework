\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,bm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{placeins}
\setlength{\parskip}{1em} %Paragraph spacing
\usepackage[ruled]{algorithm2e} % Algorithms 
\usepackage{natbib}

\DeclareSymbolFont{matha}{OML}{txmi}{m}{it}% txfonts
\DeclareMathSymbol{\vv}{\mathord}{matha}{118}
 
\begin{document}
  
\title{Visual Understanding 1 Coursework}
\author{Garoe Dorta-Perez\\
CM50248: Visual Understanding 1\\
Unit Leader: Peter Hall\\
Bath University}
 
\maketitle
 
\section{Introduction}

The main objectives of this unit are to acquire some fundamental knowledge in Computer Vision.
In order to gather this skills we will attempt to build a stereo reconstruction system.
We refer the readers to \cite{Hartley2004} book on reconstructing a real world scene given several images of it, for an in depth treatise of the topics covered in this report.
Using a bottom-up approach the following steps will be taken:
\begin{enumerate}
\item \textbf{Image convolution:} transforming using kernel matrices.
\item \textbf{Feature construction:} detecting scale invariant interest points in images.
\item \textbf{Image matching:} using the previous points to match two images.
\item \textbf{Stereo reconstruction:} rebuilding 3D points from 2D matched points in stereo images.
\end{enumerate}

\section{Convolution}
\label{sec:convolution}

Transforming images has artistic and practical applications, for instance blurring certain parts in a person face to hide defects or detecting borders with differential operators.
For simplicity we will restrict ourselves to gray images, defined as two-dimensional $n \times m$ matrices where each element represents the intensity, and is in the range $\lbrace  0, 1 \rbrace$ where 0 is black and 1 is white.
A convolution kernel $k$, is defined as a $k_1 \times k_2$ matrix, such that $k_1 \leqslant n$ and $k_2 \leqslant m$.
When an image $I$ is convoluted with a kernel, a new image $I'$ is generated, such that each element in the second image is calculated as follows:

\begin{equation}
I'(i,j) = I(i,j) * k = \sum_{n = -N}^N \sum_{m = -M}^M k(n + N, m + M) I(n + i,m + j),
\end{equation}

where $*$ is the convolution operator, $i$ and $j$ are the pixel indices in the image, $N = \frac{1}{2}k_1$, $M = \frac{1}{2}k_2$; since $n$ and $m$ are also indices, the rounding mechanism for $N$ and $M$ becomes an implementation arbitrary criteria. 
The advantage of this definition lies in the characterization of a flexible framework for several transformations; if most cases it will only be necessary to place the right values in the kernel to achieve the desired transformation.
However, we need to specify what happens in the edges of the matrix, for instance copying them in the output image, computing the values circularly, setting them to zero, etc.

An interesting property of convolutions is the \emph{convolution theorem}, which states:

\begin{equation}
g * h = F^{-1} ( F(g)  F(h) ),
\end{equation}

where $g$ and $h$ are the matrices to be convoluted, $F$ is the Fourier transform and $F^{-1}$ the inverse of the Fourier transform.
This property allows to transform the images into the Fourier domain, apply the convolution, transform back and obtain the same results; performing the convolution in the indicated manner is beneficial, as it is more computationally efficient.
Below we show some examples of our convolution code, as well as a table comparing the performance of our initial code, our code with the Fourier transform and Matlab's implementation.

\begin{figure}[htbp]
        \centering
        \begin{subfigure}[b]{0.32\textwidth}
                \includegraphics[width=\textwidth]{images/conv1}
                \caption{myconv.}
                \label{fig:myconv}
        \end{subfigure}
        \begin{subfigure}[b]{0.32\textwidth}
                \includegraphics[width=\textwidth]{images/conv2}
                \caption{myconvFFT.}
                \label{fig:myconvFFT}
        \end{subfigure}
        \begin{subfigure}[b]{0.32\textwidth}
                \includegraphics[width=\textwidth]{images/conv2}
                \caption{Matlab's conv2.}
                \label{fig:conv2}
        \end{subfigure}
        \caption{Comparative of the different convolution codes with a horizontal sovel kernel $k = \left[-1, 1 \right]$.}\label{fig:convolution}
\end{figure}

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|c|}
	\hline
     & myconv & myconvFFT & Matlab's conv2 \\ \hline
	Time & 0.658s   & 0.04s     & 0.002s   \\ \hline
	\end{tabular}
	\caption{Performance comparative of the convolution methods.}
	\label{tb:convolutionPerfomance}
\end{table}
 
In Figure~\ref{fig:convolution} we see that the convolution theorem holds, as the output images appear to be identical, and in Table~\ref{tb:convolutionPerfomance} we see that performing the operations in the Fourier domain and transforming back is more efficient.

\section{Features}

Being able to detect image features that are invariant to rotations, translations and changes in illumination or scale is an useful tool, with many applications such as image matching.
SIFT features, proposed by \cite{Lowe2004}, provide a fairly robust method to detect points in an image with the aforementioned characteristics.
A broad overview of the algorithm is shown beneath:
\begin{enumerate}
\item \textbf{Extrema detection:} maximum and minimum intensity points are detected in a difference-of-Gaussian pyramid built with the input image.
\item \textbf{Keypoint refinement:} point centres are interpolated and weak features are rejected.
\item \textbf{Orientation assignment:} orientation and strength of each keypoint are calculated.
\item \textbf{Descriptor calculation:} a 128 feature vector is computed for every element.
\end{enumerate}

\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{.48\textwidth}
		\centering
		\includegraphics[width=1.05\textwidth]{images/diffOfGauFromPaper}
		\caption{Pyramid of difference-of-Gaussian, \cite{Lowe2004}.}
		\label{fig:difOfGauPyramid}
	\end{minipage} ~
	\begin{minipage}[t]{.48\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{images/dogSpace}
		\caption{Example of a scale of a difference-of-Gaussian.}
		\label{fig:difOfGauSample}
	\end{minipage}
\end{figure}

\textbf{Extrema detection} is computed using a difference-of-Gaussian (DOG) structure, as shown in Figure~\ref{fig:difOfGauPyramid}, first a scale of blurred images is constructed.
The input image $I$ is convoluted with a Gaussian kernel:

\begin{equation}
L(x,y,\sigma) = G(x,y,\sigma) * I(x,y),
\end{equation}

where $L$ is the output image, $x$ and $y$ the pixel indices, and $G$ and Gaussian kernel defined as:

\begin{equation}
G(x,y,\sigma) = \frac{1}{2 \pi \sigma^2} e^{- \frac{x^2 + y^2}{2 \sigma^2}},
\end{equation}

where $\sigma$ is the standard deviation.
Next, the DOG structure is computed as a difference of several $L$:

\begin{equation}
D(x,y,\sigma) = L(x,y, k\sigma) - L(x,y, \sigma),
\end{equation}

where $D$ is the output image and $k$ is a constant multiplicative factor.
Note that to provide an efficient implementation, the convolution and difference can be computed in the Fourier domain, using the convolution theorem explained in Section~\ref{sec:convolution}.
An example on how this DOG structure looks like is shown in Figure~\ref{fig:difOfGauSample}, where sigma increases from left to right and from top to bottom.
The keypoints selected will be those who are a maximum or a minimum in a $3 \times 3 \times 3$ cube region, so in their scale and the ones above and below.
Selecting locations using this scheme allows to find keypoints regardless of their scale, as we progress towards the top of the pyramid,  the local maximum or minima will be produced by bigger objects in $I$.
In our implementation only one octave of the pyramid is calculated, as shown in Figure~\ref{fig:DoGExtrema}.

\begin{figure}[htbp!]
	\centering
	\begin{subfigure}[a]{0.59\textwidth}
		\includegraphics[width=\textwidth]{images/DoGWithExtrema}
		\caption{}
		\label{fig:DoGExtrema}
	\end{subfigure}
	\begin{subfigure}[a]{0.39\textwidth}
		\includegraphics[width=\textwidth]{images/siftHistograms}
		\caption{}
		\label{fig:histoCal}
	\end{subfigure}
	\caption{DoG space with simple extrema points detection in (a), and SIFT histogram computation in (b), \cite{Gu2013}.}
\end{figure}

Keypoint refinement is done with an optimization step which removes weak points and also improves their position.
The new keypoint location $\mathbf{\hat{x}}$ will be:

\begin{equation}
\mathbf{\hat{x}} = - \frac{\partial^2 D^{-1}}{\partial \mathbf{x^2}} \frac{\partial D}{\partial \mathbf{x}},
\end{equation}

where $\mathbf{x}$ is the previous location and $D(\mathbf{x})$ is approximated with a Taylor expansion in the form of:

\begin{equation}
D(\mathbf{x}) = D + \frac{ \partial D^T}{\partial \mathbf{x}} \mathbf{x} + 0.5 \mathbf{x}^T \frac{\partial^2 D}{\partial \mathbf{x^2}}\mathbf{x}.
\end{equation}

Keypoints are rejected if they are weak or in an edge, any extrema is defined as weak if $D(\mathbf{x}^T) < 0.03$, where $D(\mathbf{\hat{x}})$ is:

\begin{equation}
D(\mathbf{\hat{x}}) = D + 0.5 \frac{\partial D^T}{\partial \mathbf{x}}\mathbf{\hat{x}}.
\end{equation}

Edge elimination is performed when $R(\mathbf{H}) > f(r)$, where $r = 10$, $\mathbf{H}$ is a $2 \times 2$ Hessian matrix and $R(\mathbf{H})$ is defined as:

\begin{equation}
R(\mathbf{H})= \frac{ (D_{xx} + D_{yy})^2 }{ D_{xx} D_{yy} - (D_{xy})^2}, \: \text{where} \: D_{\alpha \beta} = \frac{\partial^2 D}{\partial \alpha \partial \beta}, \:  \text{for} \: \alpha, \beta = x, y,
\end{equation}

and 

\begin{equation}
f(r) = \frac{(r + 1)^2}{r}.
\end{equation}

\textbf{Orientation assignment} measures the point orientation using local image properties.
The objective is to obtain the same descriptor for a point regardless of image rotation, i.e. if a keypoint is defined by an object in a picture, we rotate the object and take another picture, the descriptor in the second picture should be identical to the first one.
A gradient magnitude $m(x,y)$ and orientation angle $\theta(x,y)$ are computed as follows:

\begin{equation}
\begin{aligned}
	m(x,y) &= \sqrt[]{(L(x+1,y,\sigma) - L(x-1,y,\sigma))^2 + (L(x,y+1,\sigma) - L(x,y-1,\sigma))^2}, \\
	\theta(x,y) &= tan^{-1} \left( \left( L(x,y+1,\sigma) - L(x, y-1, \sigma) \right) / \left( L(x+1,y,\sigma) - L(x-1, y, \sigma) \right) \right),
\end{aligned} \label{eq:conv}
\end{equation}

where $L$ is chosen to have the same sigma as the keypoint.

\textbf{Descriptor calculation} uses orientation histograms from sample points close to the extrema.
For each keypoint a $16 \times 16$ region around is selected, and the orientations for each pixel are calculated.
$4 \times 4$ histograms are computed with 8 bins each; this 8-dimensional values are placed in an array forming a 128 element feature vector for each keypoint, as shown in Figure~\ref{fig:histoCal}.
Normalizing the vector to unit length improves invariance to illumination changes.
In Figure~\ref{fig:featuresSample}, keypoints detected in a sample image by our SIFT implementation and by Matlab's SURF features, \cite{Bay2006}, are shown.

\begin{figure}[htbp!]
        \centering
        \begin{subfigure}[b]{0.49\textwidth}
                \includegraphics[width=\textwidth]{images/sift2}
                \caption{Basic SIFT result.}
                \label{fig:sift1}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
                \includegraphics[width=\textwidth]{images/sift1}
                \caption{SIFT with weak and edge keypoint removal.}
                \label{fig:sift2}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
                \includegraphics[width=\textwidth]{images/surf1}
                \caption{SURF with default parameters.}
                \label{fig:surf1}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
                \includegraphics[width=\textwidth]{images/surf2}
                \caption{SURF with the 120 strongest keypoints.}
                \label{fig:surf2}
        \end{subfigure}        
        \caption{Comparative of our SIFT features with Matlab's SURF features.}\label{fig:featuresSample}
\end{figure}

\FloatBarrier % Makes sure the previous figure does not go into the Matching section
\section{Matching}

In the image matching area, an image $I$ is to be found under a certain affine transformation in another image $I'$.
Due to camera measurement errors, changes in illumination and other factors, a global optimal solution is computationally expensive.
The RANSAC (Random Sample Consensus) algorithm is a simple and fast approach to perform this task, \cite{Fischler1981}.
Despite being originally designed to estimate the parameters that fit a mathematical model to a dataset which contains outliers, it can also be used to estimate the transformation $H$ that matches $I$ and $I'$.
Recently, researchers have proposed more efficient implementations, \cite{Dung2013}.
A general overview of the algorithm is given below:

\begin{algorithm}[H]
 \caption{RANSAC}
 \KwData{A pair of images $I$ and $I'$, number of iterations $k$ , distance threshold $t$.}
 \KwResult{Homography matrix $H$.}
 $f$, $f'$ = detectKeypoints($I$, $I'$)\;
 $m$ = matchFeatures($f$, $f'$)\;
 $H = identity$\;
 \For{$i\leftarrow 0$ \KwTo $k$ }{
  $rp$ = pickRandomPoints($m$)\;
  $newH$ = computeHomography($rp$)\;
  \If{isBetter($newH$, $H$, $t$)}{
   $H = newH$\;
   }
 }
\end{algorithm}

To detect and match the keypoints, our implementation uses SURF features.
The goal in this stage is to calculate the matrix $H$ that transforms from one image to the other, such that:

\begin{equation}
\begin{bmatrix}
u \\ 
\vv \\ 
w
\end{bmatrix}
= 
\begin{bmatrix}
 h_{11} & h_{12} & h_{13} \\ 
 h_{21} & h_{22} & h_{23}\\ 
 h_{31} & h_{32} & h_{33}
\end{bmatrix}
\begin{bmatrix}
u' \\
\vv' \\
w'
\end{bmatrix},
\label{eq:originalH}
\end{equation}

where $u$, $\vv$ and $w$ are the scaled pixel coordinates in $I$ and $u'$, $\vv'$ and $w'$ the scaled pixel coordinates in $I'$.
Collecting $H$ in rows, $H_i = \begin{bmatrix} h_{i1} & h_{i2} & h_{i3} \end{bmatrix}$ for $i = \lbrace 1,2,3 \rbrace$ and $\mathbf{u'} = \begin{bmatrix} u' & \vv' & w' \end{bmatrix}$ we can rewrite Equation~\ref{eq:originalH} into:

\begin{equation}
\begin{aligned}
	u = H_1 \mathbf{u'}, \: \vv &= H_2 \mathbf{u'}, \: w = H_3 \mathbf{u'}, \\ 
	\text{enforcing} \: w &= 1, \\
	u^* = \frac{H_1 \mathbf{u'}}{ H_3 \mathbf{u'}}, \: \vv^* &= \frac{H_2 \mathbf{u'}}{ H_3 \mathbf{u'}}, \: \text{hence} \\	
	%
	u^*H_3\mathbf{u'} - H_1\mathbf{u'} &= 0, \\
	\vv^* H_3\mathbf{u'} - H_2\mathbf{u'} &= 0, \: thus\\
	%
	\begin{bmatrix}
	-\mathbf{u'} & \mathbf{0} & u^*\mathbf{u'} \\
	\mathbf{0} & -\mathbf{u'} & \vv^*\mathbf{u'}
	\end{bmatrix}
	%
	\begin{bmatrix}
	H_1^T \\
	H_2^T \\ 
	H_3^T 
	\end{bmatrix}
	&= \mathbf{0}^T,
\end{aligned}
\label{eq:derivatingH}
\end{equation}

where $\mathbf{0} = \begin{bmatrix} 0 & 0 & 0 \end{bmatrix}$. 
As shown above, each match provides two equations to solve $H$.
Since we enforce the scale to be homogeneous, $h_{33} = 1$ is no longer an unknown.
For eight unknowns, eight equations are needed, so four points in each image are to be matched.
In matrix notation, we will stack all the equations in a matrix $A$, such that $A\mathbf{h}=0$, where $\mathbf{h}$ is $H$ reshaped into a column vector.
Singular value decomposition of $A$ can be computed to solve the system, $A = UZV^T$, where $U$ and $V$ are unitary, $Z$ is rectangular diagonal and $\mathbf{h} = V_{i9}$ for $i = \lbrace 1,2, \ldots, 9 \rbrace$.

The metric to define if a $newH$ is better than a certain $H$ is double; each matched keypoint in $I$ is transformed with $newH$, the euclidean distance $d_i$ between the transformed keypoint and its match in $I'$ is calculated, and if it is below a threshold $t$, we count it as a good match.
$newH$ is considered an improvement over $H$ if it generates more good matches, or the same number although with lower overall $D$, such that $D = \sum d_i $.
Some results obtained with our code are shown in Figure~\ref{fig:ransacSample}.

\begin{figure}[htbp!]
        \centering
        \begin{subfigure}[b]{0.8\textwidth}
                \includegraphics[width=\textwidth]{images/ransac2}
                \caption{A synthetic test.}
                \label{fig:ransac1}
        \end{subfigure}    
\end{figure}

\begin{figure}[htbp]
		\ContinuedFloat
		\centering        
        \begin{subfigure}[b]{0.8\textwidth}
                \includegraphics[width=\textwidth]{images/ransac1}
                \caption{Matching a box, images taken from \cite{Lowe2004}.}
                \label{fig:ransac2}
        \end{subfigure} 
        \caption{Samples of $H$ computation, left is $I$, centre is $I'$ and right is $I * H$. Keypoints matching between input images is shown for clarity.}\label{fig:ransacSample}
\end{figure}

\FloatBarrier
\section{Reconstruction}

3D stereo reconstruction from a pair of cameras involves three steps:
\begin{enumerate}
\item \textbf{Camera calibration:} compute camera model parameters.
\item \textbf{Fundamental matrix:} estimate an $F$ matrix such that $\mathbf{u}'^TF\mathbf{u} = 0$, where $\mathbf{u}$ and $\mathbf{u}'$ are corresponding points in two images.
\item \textbf{Reconstruct 3D points:} once constrained by $F$, triangulate points position from the two images.
\end{enumerate}

We will model our cameras using the pinhole camera model, in this framework a world 3D point $\mathbf{x} = \begin{bmatrix} x & y & z & 1 \end{bmatrix}$ is projected into a 2D image point $\mathbf{u} = \begin{bmatrix} u & \vv & 1 \end{bmatrix}$ using the following equations:

\begin{gather}
	\mathbf{u}^T = P \mathbf{x}^T, \label{eq:matrixP} \\
	\mathbf{u}^T = K [R | \mathbf{t}] \mathbf{x}^T, \label{eq:matrixRT}
\end{gather}

where $K$ is 
\begin{equation}
\begin{bmatrix}
f_x & 0 & p_x \\ 
0 & f_y & p_y \\ 
0 & 0 & 1
\end{bmatrix}
\end{equation}

with $f_x$ and $f_y$ are focal length parameters and $p_x$ and $p_y$ are the coordinate of the principal point, $P$ is a $3 \times 4$ projection matrix, $R$ is a $3 \times 3$ rotation matrix, $t$ is a translation column vector, making $[R|t]$ a $3 \times 4$ matrix.
Note that $\mathbf{x}$ and $\mathbf{u}$ have a homogeneous coordinate.
$K$ is commonly referred as camera intrinsics matrix, $[R|\mathbf{t}]$ is called camera extrinsics matrix, and the estimation of their elements is known as \textbf{camera calibration}.

Equation~\ref{eq:matrixP} can be solved following equivalent steps as to those used in Equations~\ref{eq:originalH} and~\ref{eq:derivatingH}, although with an extra coordinate in $\mathbf{x}$, and regarding $P$ as equivalent to $H$ with an added column.
Therefore we can again stack several points in a matrix $A$, yet the increased number of unknowns determine that six points are needed to estimate $P$,

\begin{equation}
A\mathbf{p} = 0,
\end{equation}

where $\mathbf{p}$ is $P$ reshaped into a column vector.
In the aforementioned technique, the use of a calibration cube is implicit in order to gather the six input points needed.
Nevertheless, there are more flexible techniques, such as \cite{Zhang2000} approach to calibrate cameras using at least two images of a calibration plane taken at different camera orientations.

Once we have solved the aforementioned equation with SVD, we need to decompose $P$ into $K[R|t]$.
Since $K$ is an upper triangular matrix, we can use an an $RQ$ decomposition on $P$, where $R$ is upper triangular and $Q$ is orthogonal, \cite{Francis1961}.
For the decomposition only the first three columns in $P$ will be used, and $\mathbf{t} = K^{-1}P_4$, where $P_4$ is the last column in $P$.

% Figure with calibration object???

\begin{figure}[htbp!]
	\centering
	\includegraphics[width=0.4\textwidth]{images/epipolarLines}
	\caption{A point $\mathbf{x}$ in the left image is mapped via a plane $\boldsymbol{\pi}$ to a point $\mathbf{x}'$ in the right image, \cite{Hartley2004}.}\label{fig:epipolarLines}
\end{figure}

Epipolar lines and the \textbf{fundamental matrix} $F$ are useful tools in stereo reconstruction.
In Figure~\ref{fig:epipolarLines}, the geometrical relationship of the projection of a 3D world point $\mathbf{X}$ in two camera images is shown.
We define $\mathbf{e}$ and $\mathbf{e}'$ as the epipolar points (epipoles), $F$ is a matrix that satisfies $\mathbf{u}'^TF\mathbf{u} = 0$, where $\mathbf{u} = \begin{bmatrix} u & \vv & 1 \end{bmatrix}$ and $\mathbf{u}' = \begin{bmatrix} u' & \vv' & 1 \end{bmatrix}$ are matching points in the left and right images respectively, and $\mathbf{l}'$ is a line such that $\mathbf{l}' = F\mathbf{u}'$.

To calculate $F$ the approach in Equations~\ref{eq:originalH} and~\ref{eq:derivatingH} can be used again.
However, in the equation $A\mathbf{f} = 0$, $A$ will have following configuration:

\begin{equation}
\begin{bmatrix}
u_1 u_1' & 	\vv_1 u_1' & u_1' & u_1 \vv_1' & \vv_1 \vv_1' & \vv_1' & u_1 & \vv_1 & 1 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots  \\
u_8 u_8' & \vv_8 u_8' & u_8' & u_8 \vv_8' & \vv_8 \vv_8' & \vv_8' & u_8 & \vv_8 & 1 
\end{bmatrix}
\begin{bmatrix}
F_1^T \\
F_2^T \\
F_3^T \\
\end{bmatrix}
= \mathbf{0}^T,
\end{equation}

where the $\mathbf{f}$ is $F$ reshaped in a column vector; $F$ can be estimated using SVD.

Since we have calculated all the necessary parameters for each camera and their relationship, we can now proceed with stereo reconstruction.
An implementation in pseudocode is presented below:

\begin{algorithm}[H]
 \caption{Stereo Reconstruction}
 \KwData{A pair of images $I$ and $I'$, projection matrices for two cameras $P$ and $P'$.}
 \KwResult{Cloud of 3D points $\mathbf{X}$.}
 $f$, $f'$ = detectKeypoints($I$, $I'$)\;
 $m$ = matchFeatures($f$, $f'$)\;
 \For{$i\leftarrow 1$ \KwTo $numMatches$ }{
	$A$ = constructA($m(i)$)\;
	$X(i)$ = SVD($A$)\;
 }
 displayCloud(X)\;
\end{algorithm}

For this task we have a set of correspondences:

\begin{equation}
 \mathbf{u}_{i} = P\mathbf{x}_i, \quad \mathbf{u'}_{i} = P'\mathbf{x}_i,
\end{equation}

where the unknown is the 3D world object position $\mathbf{x}_i$.
Solving for a single point $\mathbf{x}$, we cluster the rows of $P$, such that $P_i = \begin{bmatrix} p_{i1} & p_{i2} & p_{i3} & p_{i4}  \end{bmatrix}$ for $i = \lbrace 1, 2, 3 \rbrace$,

\begin{equation}
 u = P_1\mathbf{x}, \quad \vv = P_2\mathbf{x}, \quad 1 = P_3\mathbf{x},\\
\end{equation}

multiplying the first and second equation by the third,

\begin{equation}
\begin{aligned}
	uP_3\mathbf{x} &=  P_1\mathbf{x} \: \rightarrow  \: uP_3\mathbf{x} - P_1\mathbf{x} = 0,\\
	\vv P_3\mathbf{x} &= P_2\mathbf{x} \: \rightarrow \: \vv P_3\mathbf{x} - P_2\mathbf{x} = 0,
\end{aligned}
\end{equation}

doing the equivalent for the right camera equation, and stacking them in a matrix,

\begin{equation}
	\begin{bmatrix}
	uP_3 - P_1 \\
	\vv P_3 - P_2 \\
	u'P_3' - P_1' \\
	\vv' P_3' - P_2'	
	\end{bmatrix}
	%
	\mathbf{x}^T
	= \mathbf{0}^T \: \rightarrow  \: A\mathbf{x}^T = \mathbf{0}^T.
\end{equation}

We can calculate $\mathbf{x}$ via singular value decomposition; for each point $\mathbf{x}_i$ a new $A_i$ matrix will be computed and the system will be solved.
Our implementation extracts the matches using SURF keypoints, however this produces a sparse reconstruction, as only a small subset of points in the image is matched.
In Figure~\ref{fig:stereoReconstruction}, the work flow of the reconstruction algorithm is shown with an synthetic example.
The scene (modelled using Blender) contains some books on a shelf, with a textured plane on the background, based on \cite{Archemi2014}.
The cameras position can be seen on the upper sections of Figures~\ref{fig:blender1} and~\ref{fig:blender2}.
The matching algorithm produces 87 points for this scene, however only 75 are rendered due to some erroneous matches being discarded.
Rough forms can be seen in Figure~\ref{fig:stereoRecons2}, the two blue circles depict both cameras position; the cloud has three distinctive clusters, the left one shows the points on the background, the centre-top one represents the points on the side-covers of the books, and the lower line-shaped cluster displays the points tracked on the background under the shelf.

\begin{figure}[htbp]
        \centering
        \begin{subfigure}[b]{0.35\textwidth}
                \includegraphics[width=\textwidth]{images/blender3}
                \caption{Right view of our 3D scene.}
                \label{fig:blender1}
        \end{subfigure}
        \begin{subfigure}[b]{0.35\textwidth}
                \includegraphics[width=\textwidth]{images/blender6}
                \caption{Left view of our 3D scene.}
                \label{fig:blender2}
        \end{subfigure}
\end{figure}

\begin{figure}[htbp]
		\ContinuedFloat
		\centering
        \begin{subfigure}[b]{0.49\textwidth}
                \includegraphics[width=\textwidth]{images/reconstructionMatching}
                \caption{Keypoints matching from each camera view.}
                \label{fig:stereoMatching}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
                \includegraphics[width=\textwidth]{images/reconstruction1}
                \caption{Right view of the reconstructed points.}
                \label{fig:stereoRecons1}
        \end{subfigure}
\end{figure}

\begin{figure}[htbp]
		\ContinuedFloat
		\centering        
        \begin{subfigure}[b]{0.49\textwidth}
                \includegraphics[width=\textwidth]{images/reconstruction3}
                \caption{Left view of the reconstructed points.}
                \label{fig:stereoRecons2}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
                \includegraphics[width=\textwidth]{images/reconstruction2}
                \caption{Central view of the reconstructed points.}
                \label{fig:stereoRecons3}
        \end{subfigure}
        \caption{Reconstructing 3D geometry from a simple scene with books on a shelf.}\label{fig:stereoReconstruction}
\end{figure}

\FloatBarrier
\section{Conclusion and future work}

We have presented mathematical derivations and algorithm implementations for convolution, keypoint feature detection, image matching, camera calibration and 3D stereo reconstruction.
The next steps would be centred on improving the stereo reconstruction, in order to achieve a denser reconstruction, more points have to be matched, a simple approach to achieve it, is to calculate epipolar lines of matching points, and then performing a neighbour pixel search along the line.
Furthermore, to overcome lens distortions in the cameras, and inherent numeric errors in the calibration methods, the epipolar lines can be forced to be horizontal, and recalibration can be done based on matching information.

\bibliographystyle{apalike}
\bibliography{vuBib}

\end{document}